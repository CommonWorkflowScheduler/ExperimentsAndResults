{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "cluster=\"CPU\"\n",
    "sns.set(rc={'figure.figsize':(8,4.5)})\n",
    "sns.set(rc=sns.axes_style(\"darkgrid\"))\n",
    "order = ['original', \n",
    "       'fifo_roundrobin', 'fifo_random', 'fifo_fair', \n",
    "       'random_roundrobin', 'random_random', 'random_fair', \n",
    "       'max_roundrobin', 'max_random', 'max_fair',\n",
    "       'min_roundrobin', 'min_random', 'min_fair', \n",
    "       'rank_roundrobin', 'rank_random', 'rank_fair', \n",
    "       'rank_min_roundrobin', 'rank_min_random', 'rank_min_fair', \n",
    "       'rank_max_roundrobin', 'rank_max_random', 'rank_max_fair']\n",
    "workflowNames = ['rnaseq','sarek','chipseq','atacseq','mag','ampliseq','nanoseq','viralrecon','eager']\n",
    "workflowNameMap = { \n",
    "    \"rnaseq\" : \"RNA-Seq\", \n",
    "    \"sarek\" : \"Sarek\", \n",
    "    \"chipseq\" : \"ChiP-Seq\", \n",
    "    \"atacseq\": \"ATAC-seq\", \n",
    "    \"mag\": \"MAG\", \n",
    "    \"ampliseq\": \"AmpliSeq\", \n",
    "    \"methylseq\": \"MethylSeq\", \n",
    "    \"nanoseq\": \"NanoSeq\", \n",
    "    \"viralrecon\": \"Viralrecon\", \n",
    "    \"nanoseq\": \"NanoSeq\",\n",
    "    \"eager\": \"Eager\",\n",
    "}\n",
    "prioNamesMap = { \n",
    "    \"fifo\" : \"FIFO\",\n",
    "    \"random\" : \"Ran\",\n",
    "    \"max\" : \"Size Desc\",\n",
    "    \"min\" : \"Size Asc\",\n",
    "    \"rank\" : \"Rank (FIFO)\",\n",
    "    \"rank_max\" : \"Rank (Max)\",\n",
    "    \"rank_min\" : \"Rank (Min)\"\n",
    "}\n",
    "assignNamesMap = {\n",
    "    \"roundrobin\" : \"RR\",\n",
    "    \"random\" : \"Ran\",\n",
    "    \"fair\" : \"Fair\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapStrategy( strategy ):\n",
    "    if ( strategy == \"original\" ):\n",
    "        return \"Original\"\n",
    "    else:\n",
    "        prio = strategy[:strategy.rindex(\"_\")]\n",
    "        assign = strategy[strategy.rindex(\"_\")+1:]\n",
    "        return prioNamesMap[prio] + \"-\" + assignNamesMap[assign]\n",
    "\n",
    "def runtimeFromTrace( trace ):\n",
    "    start = trace[\"submit\"].min()\n",
    "    end = trace[\"complete\"].max()\n",
    "    return ( end - start ) / 1000\n",
    "\n",
    "def readTraceFile( pathToTraceFile ):\n",
    "    with open( pathToTraceFile, \"r\") as trace:\n",
    "        lines = trace.readlines()\n",
    "        # map lines\n",
    "        lines = list( map( lambda line: line.replace(\"taxa:mitochondria,chloroplast;min-freq:10;min-samples:2\", \"taxa:mitochondria,chloroplast,min-freq:10,min-samples:2\"), lines ) )\n",
    "    #write lines into tmp file\n",
    "    with open( pathToTraceFile + \".tmp\", \"w\") as trace:\n",
    "        trace.writelines( lines )\n",
    "        result = pd.read_csv( pathToTraceFile + \".tmp\", sep=\";\" )\n",
    "    os.remove( pathToTraceFile + \".tmp\" )\n",
    "    return result\n",
    "\n",
    "def makespanFromLogs( lines ):\n",
    "    startDate = -1\n",
    "    endDate = -1\n",
    "    for l in lines:\n",
    "        if ( startDate == -1 and \"] Submitted process > \" in l and \" [Task submitter] INFO  nextflow.Session - [\" in l ):\n",
    "            startDateStr = l[:l.index(\" [\")]\n",
    "            startDate = datetime.datetime.strptime(startDateStr, '%b-%d %H:%M:%S.%f')\n",
    "        if ( \"[Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[\" in l ):\n",
    "            endDateStr = l[:l.index(\" [\")]\n",
    "            endDate = datetime.datetime.strptime(endDateStr, '%b-%d %H:%M:%S.%f')\n",
    "    return (endDate - startDate).total_seconds()\n",
    "\n",
    "\n",
    "def readRun( pathToRun ):\n",
    "    try:\n",
    "        overallRuntime = -1\n",
    "        # Overall Runtime from nextflow.log\n",
    "        with open( pathToRun + \"nextflow.log\", \"r\") as nextflowlog:\n",
    "            lines = nextflowlog.readlines()\n",
    "            firstLine = lines[0]\n",
    "            startDateStr = firstLine[:firstLine.index(\" [\")]\n",
    "            startDate = datetime.datetime.strptime(startDateStr, '%b-%d %H:%M:%S.%f')\n",
    "            lastLine = lines[-1]        \n",
    "            endDateStr = lastLine[:lastLine.index(\" [\")]\n",
    "            endDate = datetime.datetime.strptime(endDateStr, '%b-%d %H:%M:%S.%f')\n",
    "            overallRuntime = (endDate - startDate).total_seconds()\n",
    "            makespanFromLog = makespanFromLogs( lines )\n",
    "        # Read the trace file\n",
    "        trace = readTraceFile( pathToRun + \"trace.csv\" )\n",
    "        avgTaskRuntime = trace[\"realtime\"].mean() / 1000\n",
    "        medianTaskRuntime = trace[\"realtime\"].median() / 1000\n",
    "        makespan = runtimeFromTrace( trace )\n",
    "        queueDelay = (trace[\"start\"] - trace[\"submit\"]).mean() / 1000\n",
    "    except Exception as e:\n",
    "        print(\"Error reading run \" + pathToRun)\n",
    "        raise e\n",
    "\n",
    "    return { \"overallRuntime\" : overallRuntime, \"trace\" : trace, \"makespan\" : makespan, \"makespanFromLogs\" : makespanFromLog, \"avgTaskRuntime\" : avgTaskRuntime, \"medianTaskRuntime\" : medianTaskRuntime, \"taskCount\" : len(trace), \"queueDelay\" : queueDelay }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readResults( cluster ):\n",
    "    resultDir = cluster + \"/\"\n",
    "    workflows = os.listdir(resultDir)\n",
    "    resultsList = []\n",
    "    results = {}\n",
    "    for workflow in workflows:\n",
    "        results[workflow] = {}\n",
    "        strategies = os.listdir( resultDir + workflow )\n",
    "        for strategy in strategies:\n",
    "            results[workflow][strategy] = {}\n",
    "            runs = os.listdir( resultDir + workflow + \"/\" + strategy )\n",
    "            if ( len(runs) < 5 ):\n",
    "                print(\"Only found \" + str(len(runs)) + \" runs for \" + workflow + \" \" + strategy + \".\")\n",
    "            for run in runs:\n",
    "                result = readRun( resultDir + workflow + \"/\" + strategy + \"/\" + run + \"/\" )\n",
    "                results[workflow][strategy][run] = result\n",
    "                assign = \"\"\n",
    "                advanced = True\n",
    "                prio = \"\"\n",
    "                if ( strategy == \"original\"):\n",
    "                    assign = \"original\"\n",
    "                    prio = \"original\"\n",
    "                    advanced = False\n",
    "                else:\n",
    "                    assign = strategy[strategy.rindex(\"_\")+1:]\n",
    "                    prio = strategy[:strategy.rindex(\"_\")]\n",
    "                    advanced = True\n",
    "\n",
    "                resultsList.append( { \n",
    "                    \"workflow\" : workflow, \n",
    "                    \"strategy\" : strategy, \n",
    "                    \"run\" : run, \n",
    "                    \"overallRuntime\" : result[\"overallRuntime\"], \n",
    "                    \"makespan\" : result[\"makespan\"], \n",
    "                    \"makespanFromLogs\" : result[\"makespanFromLogs\"], \n",
    "                    \"avgTaskRuntime\" : result[\"avgTaskRuntime\"],\n",
    "                    \"medianTaskRuntime\" : result[\"medianTaskRuntime\"],\n",
    "                    \"assign\" : assign, \n",
    "                    \"prio\" : prio, \n",
    "                    \"advanced\" : advanced,\n",
    "                    \"taskCount\" : result[\"taskCount\"],\n",
    "                    \"queueDelay\" : result[\"queueDelay\"]\n",
    "                    } )\n",
    "\n",
    "    resultsDf = pd.DataFrame( resultsList )\n",
    "    return resultsDf, workflows, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsDf,workflows,rawdata = readResults( cluster )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumHoursRuntime = resultsDf['overallRuntime'].sum() / 60 / 60\n",
    "daysRuntime = int (sumHoursRuntime / 24)\n",
    "hoursRuntime = sumHoursRuntime % 24\n",
    "print(\"Total runtime: \" + str(daysRuntime) + \" days \" + str(hoursRuntime) + \" hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateChange( row, df, metric ):\n",
    "    # row workflow is x and row strategy is y\n",
    "    t = df[df[\"workflow\"] == row[\"workflow\"]]\n",
    "    t = t[t[\"strategy\"] == \"original\"]\n",
    "    return (row[metric] - t[metric].median()) / t[metric].median() * 100\n",
    "\n",
    "def mapYLabel( y, _ ):\n",
    "    if y == 0 :\n",
    "        return \"Median\"\n",
    "    elif y < 0:\n",
    "        return \"-\" + str(y * -1) + \"%\"\n",
    "    else:\n",
    "        return \"+\" + str(y) + \"%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotAll( data, yAxis = \"overallRuntime\", yLabel = \"y\", col_wrap = 3 ):\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    resultsDf2 = data\n",
    "    resultsDf2[\"yAxis\"] = [ calculateChange( row, resultsDf2, yAxis ) for index, row in resultsDf2.iterrows() ]\n",
    "\n",
    "    plot = sns.FacetGrid(resultsDf2, col=\"workflow\", height=(4 if col_wrap==3 else 6), aspect=1, col_wrap = col_wrap, sharey=False, col_order=workflowNames )\n",
    "    orderNew = order.copy()\n",
    "    orderNew.insert(0, \"original\")\n",
    "    plot.map(sns.boxplot, \"strategy\", \"yAxis\", order=orderNew, medianprops=dict(zorder=110) )\n",
    "\n",
    "    #tab10 color palette https://stackoverflow.com/questions/64369710/what-are-the-hex-codes-of-matplotlib-tab10-palette\n",
    "    counter = 0\n",
    "    for axes in plot.axes.flat:\n",
    "        _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=90)\n",
    "\n",
    "\n",
    "        axes.artists.remove(axes.artists[1])\n",
    "        for i in range(6):\n",
    "            axes.lines.remove(axes.lines[6])\n",
    "\n",
    "        for i,box in enumerate(axes.artists):\n",
    "            strategy = order[i]\n",
    "            if ( strategy == \"original\" ):\n",
    "                box.set_facecolor('#7f7f7f')\n",
    "            elif ( strategy.startswith(\"fifo\") ):\n",
    "                box.set_facecolor('#ff7f0e')\n",
    "            elif ( strategy.startswith(\"random\") ):\n",
    "                box.set_facecolor('#2ca02c')\n",
    "            elif ( strategy.startswith(\"max\") ):\n",
    "                box.set_facecolor('#d62728')\n",
    "            elif ( strategy.startswith(\"min\") ):\n",
    "                box.set_facecolor('#9467bd')\n",
    "            elif ( strategy.startswith(\"rank_min\") ):\n",
    "                box.set_facecolor('#e377c2')\n",
    "            elif ( strategy.startswith(\"rank_max\") ):\n",
    "                box.set_facecolor('#1f77b4')\n",
    "            elif ( strategy.startswith(\"rank\") ):\n",
    "                box.set_facecolor('#8c564b')\n",
    "            box.zorder = 100\n",
    "        cWorkflow = resultsDf2[resultsDf2[\"workflow\"] == workflowNames[counter]]\n",
    "        orig = cWorkflow[ cWorkflow[\"strategy\"] == \"original\" ][yAxis]\n",
    "        axes.axhline(y=0, color='black', linestyle='-', linewidth=1, zorder = 1, label=\"Median original run\")\n",
    "        axes.axhline(y=(orig.min() - orig.median()) / orig.median() * 100, color='black', linestyle='dotted', linewidth=1, zorder = 1, label=\"Best original run\")\n",
    "        labels = [mapStrategy(item.get_text()) for item in axes.get_xticklabels()]\n",
    "        if( len(labels) > 0 ):\n",
    "            labels[1] = \"\"\n",
    "        axes.set_xticklabels( labels )\n",
    "        plt.setp(axes.get_xticklabels()[:1], fontweight=\"bold\")\n",
    "        axes.yaxis.set_major_formatter(mapYLabel)\n",
    "\n",
    "        axes.set_ylabel( yAxis )\n",
    "        handles, labels = axes.get_legend_handles_labels()\n",
    "        axes.title.set_text( workflowNameMap[workflowNames[counter]] )\n",
    "        counter += 1\n",
    "\n",
    "    \n",
    "    plot.set_xlabels(\"Scheduling strategy\")\n",
    "    plot.set_ylabels(yLabel)\n",
    "    plot.add_legend( {labels[i]: handles[i] for i in range(len(labels))} )\n",
    "    sns.move_legend(\n",
    "        plot, \"lower center\",\n",
    "        bbox_to_anchor=(.5, -0.03), ncol=3, title=None, frameon=True,\n",
    "    )\n",
    "    additional = \"\"\n",
    "    if ( col_wrap != 3 ):\n",
    "        additional = \"_\" + str(col_wrap)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig( \"plots/\" + cluster + \"_\" + yAxis + additional + \".jpg\", dpi = 100, bbox_inches=\"tight\" )\n",
    "    plt.savefig( \"plots/\" + cluster + \"_\" + yAxis + additional + \".pdf\", dpi = 100, bbox_inches=\"tight\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotAll( resultsDf, \"overallRuntime\", \"Runtime change compared\\nto original median\" )\n",
    "plotAll( resultsDf, \"makespanFromLogs\", \"Makespan\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsDf[\"runtimeMakespanDiff\"] = resultsDf[\"overallRuntime\"] - resultsDf[\"makespan\"]\n",
    "resultsDf[\"runtimeMakespanFromLogsDiff\"] = resultsDf[\"overallRuntime\"] - resultsDf[\"makespanFromLogs\"]\n",
    "(resultsDf.groupby([\"workflow\",\"advanced\"])[[\"overallRuntime\",\"makespan\",\"makespanFromLogs\",\"runtimeMakespanDiff\",\"runtimeMakespanFromLogsDiff\"]].median() / 60).round(2).sort_values(by=[\"workflow\",\"advanced\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapAdvanced( x ):\n",
    "    if x == \"True\" :\n",
    "        return \"Advanced\"\n",
    "    elif x == \"False\" :\n",
    "        return \"Original\"\n",
    "ax = sns.boxplot(x=\"advanced\", y=\"runtimeMakespanFromLogsDiff\", data=resultsDf)\n",
    "ax.set_ylabel(\"Nextflow Overhead in Seconds\")\n",
    "ax.set_xlabel(\"Scheduling Strategy\")\n",
    "labels = [mapAdvanced(item.get_text()) for item in ax.get_xticklabels()]\n",
    "ax.set_xticklabels( labels )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advanced = resultsDf[resultsDf[\"advanced\"] == True].groupby([\"workflow\"])[\"runtimeMakespanFromLogsDiff\"].median()\n",
    "original = resultsDf[resultsDf[\"advanced\"] == False].groupby([\"workflow\"])[\"runtimeMakespanFromLogsDiff\"].median()\n",
    "advancedOriginalDiff = advanced.to_frame(\"advanced\").join(original.to_frame(\"original\")).sort_values(by=\"advanced\", ascending=False)\n",
    "advancedOriginalDiff[\"diff\"] = advancedOriginalDiff[\"advanced\"] - advancedOriginalDiff[\"original\"]\n",
    "print(\"Average difference in seconds: \" + str(advancedOriginalDiff[\"diff\"].mean()))\n",
    "print(\"Median difference in seconds: \" + str(advancedOriginalDiff[\"diff\"].median()))\n",
    "print(\"Max difference in seconds: \" + str(advancedOriginalDiff[\"diff\"].max()))\n",
    "print(\"Min difference in seconds: \" + str(advancedOriginalDiff[\"diff\"].min()))\n",
    "print(\"Standard deviation in seconds: \" + str(advancedOriginalDiff[\"diff\"].std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advancedOriginalDiff[\"diff\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateChanges(df, groupbyOrder, groupyby = \"strategy\" ):\n",
    "\n",
    "    allWorkflows = df.workflow.unique()\n",
    "    attribute = \"overallRuntime\"\n",
    "\n",
    "    changes = []\n",
    "    for strategy in groupbyOrder:\n",
    "        betterThanMin = 0\n",
    "        betterThanMedian = 0\n",
    "        betterThanOrig = 0\n",
    "        medBetterThanOrigMed = 0\n",
    "        diffMin = []\n",
    "        diffWorst = []\n",
    "        diffMedian = []\n",
    "        diffMedMed = []\n",
    "        standardDeviation = []\n",
    "        queueDelay = []\n",
    "        runs = 0\n",
    "        for workflow in allWorkflows:\n",
    "            t = df[df[\"workflow\"] == workflow]\n",
    "            orig = t[ t[groupyby] == \"original\" ][attribute]\n",
    "            orig = orig.sort_values()\n",
    "            oMed = orig.median()\n",
    "            oMin = orig.min()\n",
    "            oMax = orig.max()\n",
    "            c = t[ t[groupyby] == strategy ][attribute]\n",
    "            c = c.sort_values()\n",
    "            cMed = c.median()\n",
    "            betterThanMin += (c < oMin).sum()\n",
    "            betterThanMedian += (c < oMed).sum()\n",
    "            #betterThanOrig += (c < orig).sum()\n",
    "            if ( c.median() < oMed ):\n",
    "                medBetterThanOrigMed += 1\n",
    "            # improvement of c\n",
    "            diffMedian.extend(list((c - oMed) / oMed))\n",
    "            diffMin.extend(list((c - oMin) / oMin))\n",
    "            diffWorst.extend(list((c - oMax) / oMax))\n",
    "            diffMedMed.append((c.median() - oMed) / oMed)\n",
    "            standardDeviation.append(((c - oMed) / oMed).std())\n",
    "            queueDelay.extend( list(t[ t[groupyby] == strategy ][\"queueDelay\"]) )\n",
    "            runs += len(c)\n",
    "        changes.append( { \n",
    "            \"strategy\" : strategy, \n",
    "            \"betterThanMin\" : betterThanMin / runs, # how often is the new strategy better than the best original run (in percent)\n",
    "            \"betterThanMedian\" : betterThanMedian / runs, # how often is the new strategy better than the median original run (in percent)\n",
    "            \"medBetterThanOrigMed\" : medBetterThanOrigMed / len(allWorkflows), # how often is the median of the new strategy better than the median of the original runs (in percent)\n",
    "            #\"betterThanOrig\": betterThanOrig / runs, \n",
    "            \"diffMedian\" : np.array(diffMedian).mean(), # how much better are all runs of the new strategy than the median of the original runs (average change in percent)\n",
    "            \"diffMin\" : np.array(diffMin).mean(), # how much better are all runs of the new strategy than the best of the original run (average change in percent)\n",
    "            \"diffMedMed\" : np.array(diffMedMed).mean(), # Change of the median of the strategy over the median of the original (average change in percent)\n",
    "            \"bestDiffMedMed\" : np.array(diffMedMed).min(), # Best of the median of the strategy over the median of the original (average change in percent)\n",
    "            \"worstDiffMedMed\" : np.array(diffMedMed).max(), # Worst of the median of the strategy over the median of the original (average change in percent)\n",
    "            \"diffWorstWorst\" : np.array(diffWorst).max(), # how much worst is the worst run of the new strategy than the worst of the original run (average change in percent)\n",
    "            \"diffMinMin\" : np.array(diffMin).min(), # how much better is the best run of the new strategy than the best of the original run (average change in percent)\n",
    "            \"avgStd\" : np.array(standardDeviation).mean(), # mean std of the new strategy\n",
    "            \"bestStd\" : np.array(standardDeviation).min(), # best std of the new strategy\n",
    "            \"worstStd\" : np.array(standardDeviation).max(), # worst std of the new strategy\n",
    "            \"avgQueueDelay\" : np.array(queueDelay).mean(), # mean queue delay of the new strategy\n",
    "        } )\n",
    "\n",
    "    changesDf = pd.DataFrame(changes)\n",
    "    return changesDf\n",
    "\n",
    "changesDf = calculateChanges( resultsDf, order, \"strategy\" ) \n",
    "changesDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how do the assignment strategies compare to each other\n",
    "changesAssignDf = calculateChanges( resultsDf, [\"fair\",\"random\",\"roundrobin\"], \"assign\" ) \n",
    "changesAssignDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how do the prio strategies compare to each other\n",
    "changesPrioDf = calculateChanges( resultsDf, [\"fifo\",\"random\",\"max\",\"min\",\"rank\",\"rank_max\",\"rank_min\"], \"prio\" ) \n",
    "changesPrioDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how do the workflows compare to each other\n",
    "runtimesByWorkflowAndStragegy = resultsDf[[\"workflow\",\"strategy\",\"overallRuntime\"]]\n",
    "allWorkflows = resultsDf.workflow.unique()\n",
    "results = []\n",
    "for wf in allWorkflows:\n",
    "    r = resultsDf[ resultsDf[\"workflow\"] == wf ]\n",
    "    results.append( { \"workflow\" : wf, \"std\" : (r[\"overallRuntime\"] / r[\"overallRuntime\"].mean()).std()} )\n",
    "\n",
    "pd.DataFrame(results).sort_values(\"std\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build Table with change values for Latex\n",
    "\n",
    "def formatColumn( data, column, markMax = False, markMin = False ):\n",
    "    values = []\n",
    "    valuesMapped = []\n",
    "    for strategy in order:\n",
    "        if ( strategy == \"original\" ):\n",
    "            continue\n",
    "        values.append( data[data[\"strategy\"] == strategy][column].values[0] )\n",
    "    maxValue = max(values)\n",
    "    minValue = min(values)\n",
    "    for v in values:\n",
    "        if ( (markMax and v == maxValue) or (markMin and v == minValue) ):\n",
    "            valuesMapped.append( \"\\\\textbf{\" + '{0:.1f}'.format(v * 100) + \"}\" )\n",
    "        else :\n",
    "            valuesMapped.append( '{0:.1f}'.format(v * 100) )\n",
    "    return \" & \".join( valuesMapped ) + \" \\\\\\\\\"\n",
    "\n",
    "\n",
    "firstLine = \"\\\\multicolumn{1}{c}{Metric} \"\n",
    "for o in order : \n",
    "    if ( o == \"original\" ):\n",
    "        continue\n",
    "    firstLine += \"& \\\\multicolumn{1}{c}{\\\\rot{\" + mapStrategy(o) + \"}} \"\n",
    "print(\"\\\\begin{tabular}{c*{21}{|r}}\")\n",
    "print(\"\\\\multicolumn{1}{c}{} & \\\\multicolumn{21}{c}{\\\\textbf{Strategies}} \\\\\\\\\")\n",
    "print(firstLine + \"\\\\\\\\\")\n",
    "print(\"\\\\hline\")\n",
    "print(\"Better med. & \" + formatColumn( changesDf, \"betterThanMedian\", True, False ) )\n",
    "print(\"Better min. & \" + formatColumn( changesDf, \"betterThanMin\", True, False ) )\n",
    "print(\"Med. better med. & \" + formatColumn( changesDf, \"medBetterThanOrigMed\", True, False ) )\n",
    "print(\"Med. change (avg) & \" + formatColumn( changesDf, \"diffMedian\", False, True ) )\n",
    "print(\"Min change (avg) & \" + formatColumn( changesDf, \"diffMin\", False, True ) )\n",
    "print(\"Med. med. change (avg) & \" + formatColumn( changesDf, \"diffMedMed\", False, True ) )\n",
    "print(\"Med. med. change (best) & \" + formatColumn( changesDf, \"bestDiffMedMed\", False, True ) )\n",
    "print(\"Med. med. change (worst) & \" + formatColumn( changesDf, \"worstDiffMedMed\", False, True ) )\n",
    "print(\"Worst diff to worst ori. & \" + formatColumn( changesDf, \"diffWorstWorst\", False, True ) )\n",
    "print(\"Max impr. to best ori. & \" + formatColumn( changesDf, \"diffMinMin\", False, True ) )\n",
    "print(\"Standard dev. (avg) & \" + formatColumn( changesDf, \"avgStd\", False, True ) )\n",
    "print(\"Standard dev. (best) & \" + formatColumn( changesDf, \"bestStd\", False, True ) )\n",
    "print(\"Standard dev. (worst) & \" + formatColumn( changesDf, \"worstStd\", False, True ) )\n",
    "print(\"\\\\hline\")\n",
    "print(\"\\\\end{tabular}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data footprint in kb, manually measured after a run\n",
    "dataFootprint = {\n",
    "    \"rnaseq\" : 507450,\n",
    "    \"sarek\" : 548964,\n",
    "    \"chipseq\" : 2699644, \n",
    "    \"atacseq\": 5929144, \n",
    "    \"mag\": 19002924, \n",
    "    \"ampliseq\": 273880, \n",
    "    \"nanoseq\": 14964553, \n",
    "    \"viralrecon\": 915517, \n",
    "    \"eager\": 2440970,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latex table with the runtimes\n",
    "import locale\n",
    "locale.setlocale(locale.LC_NUMERIC, 'English')\n",
    "metric = \"overallRuntime\"\n",
    "dataTable = \"\"\n",
    "for wf in workflowNames:\n",
    "    if ( dataTable != \"\" ):\n",
    "        dataTable += \"\\n\"\n",
    "    cwf = resultsDf[resultsDf[\"workflow\"] == wf]\n",
    "    numberOfTasks = cwf[\"taskCount\"].min()\n",
    "    runtimes = cwf.groupby(\"strategy\")[metric].median()\n",
    "    realtimeList = []\n",
    "    for strategy in rawdata[wf]:\n",
    "        for run in rawdata[wf][strategy]:\n",
    "            #print(rawdata[wf][strategy][run])\n",
    "            realtimeList.extend( list(rawdata[wf][strategy][run][\"trace\"][\"realtime\"]) )\n",
    "    medianRuntime = np.median(np.array(realtimeList)) / 1000\n",
    "    meanRuntime = np.mean(np.array(realtimeList)) / 1000\n",
    "    stdRuntime = np.std(np.array(realtimeList)) / 1000\n",
    "    bestStrategy = runtimes.idxmin()\n",
    "    change = ( runtimes.at[bestStrategy] - runtimes.at[\"original\"] ) / runtimes.at[\"original\"]\n",
    "\n",
    "    dataTable += workflowNameMap[wf] + \" & \"\n",
    "    dataTable += locale.format_string('%d', numberOfTasks, True )\n",
    "    dataTable += \" & \"\n",
    "    dataTable += locale.format_string('%.1f MB', dataFootprint[wf] / 1024, True )\n",
    "    dataTable += \" & \"\n",
    "    dataTable += mapStrategy(bestStrategy)\n",
    "    dataTable += \" & \"\n",
    "    dataTable += locale.format_string('%.1f', runtimes.at[\"original\"], True ) + \"s\"\n",
    "    dataTable += \" & \"\n",
    "    dataTable += locale.format_string('%.1f', meanRuntime, True ) + \"s\"\n",
    "    dataTable += \" & \"\n",
    "    dataTable += locale.format_string('%.1f', medianRuntime, True ) + \"s\"\n",
    "    dataTable += \" & \"\n",
    "    dataTable += locale.format_string('%.1f', stdRuntime, True ) + \"s\"\n",
    "    dataTable += \" & \"\n",
    "    dataTable += locale.format_string('%.1f', runtimes.at[bestStrategy], True ) + \"s\"\n",
    "    dataTable += \" & \"\n",
    "    dataTable += '{0:.1f}\\\\%'.format( change * -100 )\n",
    "    dataTable += \" \\\\\\\\\"\n",
    "print(\"\\\\begin{tabular}{c|r|r|c|r|r|r|r|r|r}\")\n",
    "head = [\n",
    "    \"Workflow\",\n",
    "    \"Strategy with best median run\",\n",
    "    \"Original median runtime\",\n",
    "    \"Best median runtime\",\n",
    "    \"Improvement\"\n",
    "]\n",
    "print(\"\\\\multicolumn{1}{p{1.2cm}|}{\\\\parbox[c][4em]{\\\\hsize}{\\\\centering Workflow}} & \\\\multicolumn{1}{p{1.2cm}|}{\\\\parbox[c][4em]{\\\\hsize}{\\\\centering \\\\# tasks instances}} & \\\\multicolumn{1}{p{1.5cm}|}{\\\\parbox[c][4em]{\\\\hsize}{\\\\centering generated data}} & \\\\multicolumn{1}{p{2cm}|}{\\\\parbox[c][4em]{\\\\hsize}{\\\\centering Strategy with the best median run}} & \\\\multicolumn{1}{p{1.8cm}|}{\\\\parbox[c][4em]{\\\\hsize}{\\\\centering Original median runtime}} & \\\\multicolumn{1}{p{1.2cm}|}{\\\\parbox[c][4em]{\\\\hsize}{\\\\centering Avg. task runtime}} & \\\\multicolumn{1}{p{1.2cm}|}{\\\\parbox[c][4em]{\\\\hsize}{\\\\centering Median task runtime}}  & \\\\multicolumn{1}{p{1.2cm}|}{\\\\parbox[c][4em]{\\\\hsize}{\\\\centering Standard dev. task runtime}} & \\\\multicolumn{1}{p{0.9cm}|}{\\\\parbox[c][4em]{\\\\hsize}{\\\\centering Best median runtime}} & \\\\multicolumn{1}{p{1.6cm}}{\\\\parbox[c][4em]{\\\\hsize}{\\\\centering Improvement}} \\\\\\\\\")\n",
    "print(\"\\\\hline\")\n",
    "print(dataTable)\n",
    "print(\"\\\\hline\")\n",
    "print(\"\\\\end{tabular}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part of the longest running task of sarek\n",
    "part = []\n",
    "taskName = []\n",
    "for strategy in rawdata[\"sarek\"]:\n",
    "    for run in rawdata[\"sarek\"][strategy]:\n",
    "        overallRuntime = rawdata[\"sarek\"][strategy][run][\"overallRuntime\"]\n",
    "        maximum = rawdata[\"sarek\"][strategy][run][\"trace\"][\"realtime\"].max() / 1000\n",
    "        idMax = rawdata[\"sarek\"][strategy][run][\"trace\"][\"realtime\"].idxmax()\n",
    "        task = rawdata[\"sarek\"][strategy][run][\"trace\"][\"process\"][idMax]\n",
    "        taskName.append(task)\n",
    "        part.append( maximum / overallRuntime )\n",
    "print(\"Part of longest task of sarek: \" + str(np.mean(np.array(part))))\n",
    "print(\"Task name: \" + str(np.unique(np.array(taskName))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6f5583cf1d9466b5c27e75c89cc6b383bed5736d6b16c51c8074d8690011a952"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
